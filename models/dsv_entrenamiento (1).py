# -*- coding: utf-8 -*-
"""DSV_Entrenamiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19D3S-4jcfn2WMhdm-BSbNIY52IBetnpK

#Instalar librerias
"""

import tensorflow as tf
# Forzar uso de CPU para instalaciones
tf.config.set_visible_devices([], 'GPU')

print("Instalando librer√≠as...")
!pip install -q tensorflow==2.19.0
!pip install -q seaborn
!pip install -q opencv-python
!pip install -q kagglehub
!pip install -q scipy
!pip install -q scikit-image

# Limpiar memoria inmediatamente despu√©s de instalaciones
import gc
gc.collect()
print("‚úÖ Librer√≠as instaladas y memoria limpiada")

"""# IMportar"""

print("Importando librer√≠as...")

# Configurar TensorFlow para uso eficiente de memoria
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

import numpy as np
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
import os
from PIL import Image
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import kagglehub
import zipfile
import shutil

print("‚úÖ Librer√≠as importadas correctamente")

"""# Descargar dataset"""

print("Iniciando descarga del dataset...")

with tf.device('/CPU:0'):
    try:
        dataset_path = kagglehub.dataset_download("tpapp157/earth-terrain-height-and-segmentation-map-images")
        print(f"‚úÖ Dataset descargado en: {dataset_path}")
    except Exception as e:
        print(f"‚ùå Error descargando dataset: {e}")
        # Usar dataset de respaldo si falla
        dataset_path = "/content/sample_data"

# Crear carpetas para organizar datos
terrain_dir = "/content/terrain_images"
height_dir = "/content/height_maps"
flood_mask_dir = "/content/flood_masks"

os.makedirs(terrain_dir, exist_ok=True)
os.makedirs(height_dir, exist_ok=True)
os.makedirs(flood_mask_dir, exist_ok=True)

# Verificar y organizar archivos
data_dir = os.path.join(dataset_path, "_dataset")
if not os.path.exists(data_dir):
    data_dir = dataset_path

print("Organizando archivos del dataset...")
files_processed = 0

for root, dirs, files in os.walk(data_dir):
    for f in files:
        if files_processed >= 5000:  # L√≠mite para evitar memoria
            break
        f_lower = f.lower()
        src = os.path.join(root, f)

        try:
            if f_lower.endswith("_t.png"):
                shutil.copy(src, terrain_dir)
                files_processed += 1
            elif f_lower.endswith("_h.png"):
                shutil.copy(src, height_dir)
            elif f_lower.endswith("_i2.png"):
                shutil.copy(src, flood_mask_dir)
        except Exception as e:
            continue

print(f"‚úÖ Dataset organizado:")
print(f"   - Terrain images: {len(os.listdir(terrain_dir))}")
print(f"   - Height maps: {len(os.listdir(height_dir))}")
print(f"   - Flood masks: {len(os.listdir(flood_mask_dir))}")

# Limpiar memoria
del dataset_path, data_dir
gc.collect()

"""# Dataset optimizado"""

class OptimizedTerrainDataset:
    def __init__(self, terrain_dir, height_dir, flood_mask_dir, img_size=(128, 128), max_samples=3000,
                 water_rgb=(17, 141, 215), water_tol=25):
        self.terrain_dir = terrain_dir
        self.height_dir = height_dir
        self.flood_mask_dir = flood_mask_dir
        self.img_size = img_size
        self.max_samples = max_samples
        self.water_rgb = np.array(water_rgb, dtype=np.int16)
        self.water_tol = water_tol  # tolerancia para variaciones

    def load_images_and_masks(self):
        terrain_files = sorted(os.listdir(self.terrain_dir))[:self.max_samples]
        X = []
        y = []
        print(f"Cargando {len(terrain_files)} im√°genes con tama√±o {self.img_size}...")
        for fname in tqdm(terrain_files, desc="Procesando im√°genes"):
            try:
                base_name = fname.replace("_t.png", "")
                terrain_path = os.path.join(self.terrain_dir, fname)
                height_path = os.path.join(self.height_dir, f"{base_name}_h.png")
                seg_path = os.path.join(self.flood_mask_dir, f"{base_name}_i2.png")
                if not (os.path.exists(height_path) and os.path.exists(seg_path)):
                    continue

                with tf.device('/CPU:0'):
                    # Terreno RGB
                    terrain_img = Image.open(terrain_path).convert('RGB').resize(self.img_size)
                    terrain_array = np.array(terrain_img, dtype=np.float32) / 255.0

                    # Altura
                    height_img = Image.open(height_path)
                    height_resized = height_img.resize(self.img_size, Image.NEAREST)
                    height_array = np.array(height_resized, dtype=np.float32)
                    height_norm = height_array / height_array.max() if height_array.max() > 0 else height_array
                    height_norm = np.expand_dims(height_norm, axis=-1)

                    # Canales
                    img_with_height = np.concatenate([terrain_array, height_norm], axis=-1)

                    # M√°scara corregida
                    mask = self.create_simple_mask(seg_path, height_array)

                    X.append(img_with_height)
                    y.append(mask)

            except Exception as e:
                print(f"Error procesando {fname}: {e}")
                continue
            if len(X) and len(X) % 500 == 0:
                gc.collect()

        print(f"‚úÖ Procesadas {len(X)} im√°genes v√°lidas")
        return np.array(X, dtype=np.float32), np.array(y, dtype=np.uint8)

    def create_simple_mask(self, seg_path, height_array):
        seg_rgb = np.array(Image.open(seg_path).convert('RGB').resize(self.img_size, Image.NEAREST), dtype=np.int16)

        diff = np.abs(seg_rgb - self.water_rgb)
        water_mask = (diff[..., 0] <= self.water_tol) & (diff[..., 1] <= self.water_tol) & (diff[..., 2] <= self.water_tol)

        mask = np.zeros(seg_rgb.shape[:2], dtype=np.uint8)
        mask[water_mask] = 1  # Clase 1 = Agua / Inundaci√≥n

        low_thresh = np.percentile(height_array[~water_mask], 25) if np.any(~water_mask) else np.percentile(height_array, 25)
        depression_mask = (height_array < low_thresh) & (~water_mask)
        mask[depression_mask] = 2  # Clase 2 = Depresi√≥n en tierra

        return mask

"""# Cargar datos por lotes"""

print("Iniciando carga de datos...")

with tf.device('/CPU:0'):
    dataset = OptimizedTerrainDataset(terrain_dir, height_dir, flood_mask_dir,
                                    img_size=(128, 128), max_samples=3000)
    X, y = dataset.load_images_and_masks()

print(f"Shapes finales: X {X.shape}, y {y.shape}")

# One-hot encoding
num_classes = 3
y_cat = to_categorical(y, num_classes)

# Dividir datos
X_train, X_val, y_train, y_val = train_test_split(
    X, y_cat, test_size=0.2, random_state=42 # Removed stratify
)

# Liberar memoria de arrays temporales
del X, y, y_cat
gc.collect()

print(f"‚úÖ Datos preparados:")
print(f"   - Train: {X_train.shape}, {y_train.shape}")
print(f"   - Validation: {X_val.shape}, {y_val.shape}")

# Verificar distribuci√≥n de clases (optional, can be removed if not needed without stratification)
# unique, counts = np.unique(np.argmax(y_train, axis=-1), return_counts=True)
# print("Distribuci√≥n de clases en TRAIN:", dict(zip(unique, counts)))

"""# Modelo ViT-UNet"""

def create_vit_unet_model(input_shape=(128, 128, 4), num_classes=3,
                          patch_size=8, embed_dim=64, num_heads=4,
                          mlp_dim=128, depth=6, dropout=0.1):
    import tensorflow as tf
    from tensorflow.keras import layers, models

    H, W, C = input_shape
    assert H % patch_size == 0 and W % patch_size == 0, "H y W deben ser m√∫ltiplos de patch_size"
    num_patches_h = H // patch_size
    num_patches_w = W // patch_size
    num_patches = num_patches_h * num_patches_w

    def transformer_block(x):
        # Atajo
        x1 = layers.LayerNormalization(epsilon=1e-6)(x)
        x1 = layers.MultiHeadAttention(num_heads=num_heads,
                                       key_dim=embed_dim // num_heads,
                                       dropout=dropout)(x1, x1)
        x1 = layers.Dropout(dropout)(x1)
        x = layers.Add()([x, x1])

        x2 = layers.LayerNormalization(epsilon=1e-6)(x)
        x2 = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x2)
        x2 = layers.Dropout(dropout)(x2)
        x2 = layers.Dense(embed_dim)(x2)
        x2 = layers.Dropout(dropout)(x2)
        x = layers.Add()([x, x2])
        return x

    inputs = layers.Input(shape=input_shape)

    # Patch embedding con conv (conv con kernel=stride=patch_size)
    x = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size,
                      padding='valid', use_bias=False)(inputs)  # (H/P, W/P, E)
    # A tokens (N, E)
    x = layers.Reshape((num_patches, embed_dim))(x)

    # Positional embeddings aprendibles
    pos_emb = layers.Embedding(input_dim=num_patches, output_dim=embed_dim)
    positions = tf.range(start=0, limit=num_patches, delta=1)
    x = x + pos_emb(positions)

    # Encoder Transformer
    for _ in range(depth):
        x = transformer_block(x)

    # Volver a grid (H/P, W/P, E)
    x = layers.Reshape((num_patches_h, num_patches_w, embed_dim))(x)

    # Decoder ligero (Upsampling conv)
    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)
    x = layers.Dropout(dropout)(x)

    x = layers.UpSampling2D(size=2, interpolation='bilinear')(x)  # 16->32
    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)

    x = layers.UpSampling2D(size=2, interpolation='bilinear')(x)  # 32->64
    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)

    x = layers.UpSampling2D(size=2, interpolation='bilinear')(x)  # 64->128
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(x)

    outputs = layers.Conv2D(num_classes, 1, padding='same', activation='softmax')(x)

    return models.Model(inputs, outputs, name="vit_unet_lite")

"""# Focal Losss"""

def focal_loss(gamma=2., alpha=None):
    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.math.log(y_pred)
        if alpha is not None:
            weights = alpha * tf.pow(1 - y_pred, gamma)
        else:
            weights = tf.pow(1 - y_pred, gamma)
        loss = weights * cross_entropy
        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))
    return loss

"""# Compilar modelo"""

print("Compilando modelo...")

strategy = tf.distribute.MirroredStrategy() if len(tf.config.list_physical_devices('GPU')) > 1 else tf.distribute.get_strategy()

with strategy.scope():
    # Reemplazar U-Net por ViT-UNet
    model = create_vit_unet_model(input_shape=(128, 128, 4), num_classes=3,
                                  patch_size=8, embed_dim=64, num_heads=4,
                                  mlp_dim=128, depth=6, dropout=0.1)

    alpha = tf.constant([1.0, 1.5, 2.0])
    loss_fn = focal_loss(gamma=2.0, alpha=alpha)

    model.compile(
        optimizer=Adam(1e-4),
        loss=loss_fn,
        metrics=['accuracy']
    )

model.summary()

"""# Entrenamiento"""

print("Iniciando entrenamiento...")

# Callbacks para evitar overfitting y gestionar memoria
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=8,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        patience=4,
        factor=0.5,
        min_lr=1e-7,
        verbose=1
    ),
    ModelCheckpoint(
        'best_model.h5',
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=False
    )
]

# Entrenar con batch size peque√±o
try:
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=30,  # Menos √©pocas
        batch_size=4,  # Batch peque√±o para memoria
        callbacks=callbacks,
        verbose=1
    )

    print("‚úÖ Entrenamiento completado exitosamente")

except Exception as e:
    print(f"‚ùå Error durante entrenamiento: {e}")
    print("Intentando con configuraci√≥n m√°s liviana...")

    # Configuraci√≥n de emergencia si falla la memoria
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=15,
        batch_size=2,
        callbacks=callbacks,
        verbose=1
    )

# Liberar memoria de datos de entrenamiento
del X_train, y_train, X_val, y_val
gc.collect()

"""# Evaluaci√≥n y visualizaci√≥n"""

print("Cargando mejores pesos para evaluaci√≥n...")
model.load_weights('best_model.h5')

# Cargar un peque√±o subset para evaluaci√≥n
with tf.device('/CPU:0'):
    dataset_eval = OptimizedTerrainDataset(terrain_dir, height_dir, flood_mask_dir,
                                         img_size=(128, 128), max_samples=100)
    X_eval, y_eval = dataset_eval.load_images_and_masks()
    y_eval_cat = to_categorical(y_eval, num_classes)

print("Evaluando modelo...")
val_loss, val_acc = model.evaluate(X_eval, y_eval_cat, verbose=0)
print(f"‚úÖ Evaluaci√≥n final - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}")

# Visualizar algunas predicciones
print("Generando visualizaciones...")
import matplotlib.pyplot as plt

def visualize_predictions(model, X, y_true, num_samples=3):
    indices = np.random.choice(len(X), num_samples, replace=False)

    plt.figure(figsize=(15, 5 * num_samples))

    for i, idx in enumerate(indices):
        # Predicci√≥n
        pred = model.predict(X[idx:idx+1], verbose=0)
        pred_mask = np.argmax(pred[0], axis=-1)
        true_mask = np.argmax(y_true[idx], axis=-1)

        # Visualizar
        plt.subplot(num_samples, 4, i*4 + 1)
        plt.imshow(X[idx][..., :3])
        plt.title("Input RGB")
        plt.axis('off')

        plt.subplot(num_samples, 4, i*4 + 2)
        plt.imshow(X[idx][..., 3], cmap='viridis')
        plt.title("Canal Altura")
        plt.axis('off')

        plt.subplot(num_samples, 4, i*4 + 3)
        plt.imshow(true_mask, cmap='tab10')
        plt.title("Verdadero")
        plt.axis('off')

        plt.subplot(num_samples, 4, i*4 + 4)
        plt.imshow(pred_mask, cmap='tab10')
        plt.title("Predicci√≥n")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Visualizar algunas predicciones
visualize_predictions(model, X_eval, y_eval_cat, num_samples=3)

# Liberar memoria
del X_eval, y_eval, y_eval_cat
gc.collect()

# The code from this cell has been moved to the previous cell (ZzhKCa5EsKmp) to resolve the NameError.

"""# Guardar modelos"""

print("Guardando modelos y resultados...")

# Guardar modelo completo
model.save("terrain_segmentation_model_final.h5")
print("‚úÖ Modelo completo guardado")

# Guardar arquitectura
model_json = model.to_json()
with open("terrain_segmentation_model.json", "w") as json_file:
    json_file.write(model_json)
print("‚úÖ Arquitectura del modelo guardada")

# Guardar historial de entrenamiento
import pandas as pd
history_df = pd.DataFrame(history.history)
history_df.to_csv("training_history.csv", index=False)
print("‚úÖ Historial de entrenamiento guardado")

"""# Conversi√≥n a tensorflow"""

print("Convirtiendo a TensorFlow Lite...")

try:
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    with open("terrain_segmentation_model.tflite", "wb") as f:
        f.write(tflite_model)
    print("‚úÖ Modelo TensorFlow Lite guardado")

except Exception as e:
    print(f"‚ùå Error en conversi√≥n TFLite: {e}")

"""# Descargar archivos"""

print("Preparando archivos para descarga...")

files_to_download = [
    "terrain_segmentation_model_final.h5",
    "terrain_segmentation_model.json",
    "training_history.csv",
    "terrain_segmentation_model.tflite"
]

from google.colab import files

for file_name in files_to_download:
    if os.path.exists(file_name):
        files.download(file_name)
        print(f"‚úÖ {file_name} descargado")
    else:
        print(f"‚ö†Ô∏è {file_name} no encontrado")

print("üéØ ¬°Proceso completado exitosamente!")

"""# Evaluaci√≥n completa"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import seaborn as sns
from tqdm import tqdm
import gc

print("üì• Cargando modelo y datos de evaluaci√≥n...")

# Cargar el modelo guardado
try:
    # Recreate the alpha tensor used during compilation
    alpha_for_loading = tf.constant([1.0, 1.5, 2.0])
    # Recreate the exact callable loss function used during compilation
    custom_loss_for_loading = focal_loss(gamma=2.0, alpha=alpha_for_loading)

    model = tf.keras.models.load_model("terrain_segmentation_model_final.h5",
                                      custom_objects={'loss': custom_loss_for_loading})
    print("‚úÖ Modelo cargado correctamente")
except Exception as e:
    print(f"‚ùå Error al cargar modelo: {e}")
    print("‚ö†Ô∏è  Usando modelo en memoria (si exist√≠a previamente o fall√≥ la carga)")

# Cargar conjunto de evaluaci√≥n m√°s grande
with tf.device('/CPU:0'):
    dataset_eval = OptimizedTerrainDataset(terrain_dir, height_dir, flood_mask_dir,
                                         img_size=(128, 128), max_samples=500)
    X_test, y_test = dataset_eval.load_images_and_masks()
    y_test_cat = to_categorical(y_test, num_classes=3)

print(f"üìä Conjunto de evaluaci√≥n: {X_test.shape}, {y_test.shape}")

"""# Evaluaci√≥n cuantitativa"""

print("\nüìà EVALUACI√ìN CUANTITATIVA")
print("-" * 30)

# Evaluaci√≥n est√°ndar
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"üìä Precisi√≥n general: {test_accuracy:.4f}")
print(f"üìä P√©rdida general: {test_loss:.4f}")

# Predicciones
print("üéØ Generando predicciones...")
y_pred_probs = model.predict(X_test, verbose=0, batch_size=4)
y_pred = np.argmax(y_pred_probs, axis=-1)
y_true = y_test

# Aplanar para m√©tricas por p√≠xel
y_true_flat = y_true.flatten()
y_pred_flat = y_pred.flatten()

"""# Metricas por clase"""

print("\nüéØ M√âTRICAS POR CLASE")
print("-" * 30)

class_names = ['Normal', 'Inundaci√≥n', 'Depresi√≥n']
report = classification_report(y_true_flat, y_pred_flat,
                              target_names=class_names, digits=4)
print("Classification Report:")
print(report)

# M√©tricas adicionales por clase
precision, recall, fscore, support = precision_recall_fscore_support(
    y_true_flat, y_pred_flat, average=None, labels=[0, 1, 2]
)

print("\nüìä Resumen por clase:")
for i, class_name in enumerate(class_names):
    print(f"  {class_name:10} - Precisi√≥n: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {fscore[i]:.4f}, Soporte: {support[i]}")

"""# Matriz de confusi√≥n"""

print("\nüìä MATRIZ DE CONFUSI√ìN")
print("-" * 30)

cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1, 2])

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Matriz de Confusi√≥n - Segmentaci√≥n de Terrenos')
plt.ylabel('Verdadero')
plt.xlabel('Predicci√≥n')
plt.tight_layout()
plt.show()

# Matriz normalizada
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(8, 6))
sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Matriz de Confusi√≥n Normalizada')
plt.ylabel('Verdadero')
plt.xlabel('Predicci√≥n')
plt.tight_layout()
plt.show()

"""# AN√ÅLISIS DE DISTRIBUCI√ìN DE CLASES"""

print("\nüìà DISTRIBUCI√ìN DE CLASES")
print("-" * 30)

unique_true, counts_true = np.unique(y_true_flat, return_counts=True)
unique_pred, counts_pred = np.unique(y_pred_flat, return_counts=True)

print("Distribuci√≥n real:")
for class_idx, count in zip(unique_true, counts_true):
    percentage = (count / len(y_true_flat)) * 100
    print(f"  {class_names[class_idx]:10}: {count:8d} p√≠xeles ({percentage:6.2f}%)")

print("\nDistribuci√≥n predicha:")
for class_idx, count in zip(unique_pred, counts_pred):
    percentage = (count / len(y_pred_flat)) * 100
    print(f"  {class_names[class_idx]:10}: {count:8d} p√≠xeles ({percentage:6.2f}%)")

"""# Visualizaci√≥n de predicciones"""

print("\nüé® VISUALIZACI√ìN DETALLADA DE PREDICCIONES")
print("-" * 40)

def visualize_detailed_predictions(model, X, y_true, num_samples=5):
    indices = np.random.choice(len(X), num_samples, replace=False)

    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4 * num_samples))

    if num_samples == 1:
        axes = axes.reshape(1, -1)

    for i, idx in enumerate(indices):
        # Predicci√≥n
        pred = model.predict(X[idx:idx+1], verbose=0)
        pred_mask = np.argmax(pred[0], axis=-1)
        true_mask = y_true[idx]

        # RGB
        axes[i, 0].imshow(X[idx][..., :3])
        axes[i, 0].set_title("Input RGB")
        axes[i, 0].axis('off')

        # Canal de altura
        axes[i, 1].imshow(X[idx][..., 3], cmap='viridis')
        axes[i, 1].set_title("Canal Altura")
        axes[i, 1].axis('off')

        # Verdadero
        axes[i, 2].imshow(true_mask, cmap='tab10', vmin=0, vmax=2)
        axes[i, 2].set_title("Verdadero")
        axes[i, 2].axis('off')

        # Predicci√≥n
        axes[i, 3].imshow(pred_mask, cmap='tab10', vmin=0, vmax=2)
        axes[i, 3].set_title("Predicci√≥n")
        axes[i, 3].axis('off')

        # Diferencia
        diff = np.where(true_mask != pred_mask, 1, 0)
        axes[i, 4].imshow(diff, cmap='Reds')
        axes[i, 4].set_title(f"Errores: {diff.sum()} p√≠xeles")
        axes[i, 4].axis('off')

    plt.tight_layout()
    plt.show()

    return indices

# Visualizar predicciones detalladas
sample_indices = visualize_detailed_predictions(model, X_test, y_test, num_samples=4)

"""# Analisis de confianza de las predicciones"""

print("\nüìä AN√ÅLISIS DE CONFIANZA DE LAS PREDICCIONES")
print("-" * 40)

# Calcular confianza por clase
confidences = np.max(y_pred_probs, axis=-1)
mean_confidence = np.mean(confidences)
std_confidence = np.std(confidences)

print(f"Confianza promedio: {mean_confidence:.4f} ¬± {std_confidence:.4f}")

# Distribuci√≥n de confianza por clase
plt.figure(figsize=(12, 4))

for class_idx in range(3):
    class_mask = (y_pred == class_idx)
    if np.any(class_mask):
        class_confidences = confidences[class_mask]
        plt.hist(class_confidences.flatten(), bins=50, alpha=0.7,
                label=f'{class_names[class_idx]}', density=True)

plt.xlabel('Confianza de Predicci√≥n')
plt.ylabel('Densidad')
plt.title('Distribuci√≥n de Confianza por Clase')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""# Analisis de errors comunes"""

print("\nüîç AN√ÅLISIS DE ERRORES COMUNES")
print("-" * 30)

# Encontrar los errores m√°s frecuentes
error_mask = (y_true != y_pred)
error_count = np.sum(error_mask)

print(f"Total de p√≠xeles con error: {error_count}")
print(f"Tasa de error global: {(error_count / len(y_true_flat)) * 100:.2f}%")

# An√°lisis de tipos de error
error_types = {}
for true_class in range(3):
    for pred_class in range(3):
        if true_class != pred_class:
            error_count = np.sum((y_true == true_class) & (y_pred == pred_class))
            if error_count > 0:
                error_types[(true_class, pred_class)] = error_count

print("\nTipos de error m√°s frecuentes:")
for (true_class, pred_class), count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"  {class_names[true_class]} ‚Üí {class_names[pred_class]}: {count:6d} p√≠xeles")

"""# Evaluaci√≥n de caracteristicas de imagen"""

print("\nüåÑ EVALUACI√ìN POR CARACTER√çSTICAS DE IMAGEN")
print("-" * 40)

# Analizar rendimiento por rango de alturas
height_values = X_test[..., 3].flatten()
height_bins = np.percentile(height_values, [0, 25, 50, 75, 100])

accuracies_by_height = []
for i in range(len(height_bins) - 1):
    mask = (height_values >= height_bins[i]) & (height_values < height_bins[i+1])
    if np.sum(mask) > 0:
        bin_accuracy = np.mean(y_true_flat[mask] == y_pred_flat[mask])
        accuracies_by_height.append(bin_accuracy)
        print(f"Altura [{height_bins[i]:.3f}-{height_bins[i+1]:.3f}]: {bin_accuracy:.4f} precisi√≥n")